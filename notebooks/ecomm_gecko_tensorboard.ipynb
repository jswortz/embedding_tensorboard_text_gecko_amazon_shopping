{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting batch prediction EDA with Tensor Board and Text Gecko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, List\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "LIMIT = 3000\n",
    "PROJECT_ID = \"wortz-project-352116\"\n",
    "DATASET = \"ecomm-embedding\"\n",
    "BUCKET_NAME = \"ecomm-query-product-pairs\"\n",
    "BUCKET = f\"gs://{BUCKET_NAME}\"\n",
    "USER_PROMPT = \"User query: \"\n",
    "PRODUCT_PROMPT = \"Product title: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_dataset(\"tasksource/esci\")  # , split=['train[:10%]','test[:10%]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick examination of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Total features: \", len(raw_data.column_names[\"train\"]))\n",
    "all_features = raw_data.column_names[\"train\"]\n",
    "all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guide for embedding fine tuning\n",
    "\n",
    "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data = load_dataset(\"tasksource/esci\", split=[\"train[:20%]\", \"test[:10%]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_jsonl_from_raw_data(\n",
    "    jsonl_path: str,\n",
    "    id_field: str,\n",
    "    text_field: str,\n",
    "    raw_data=raw_data,\n",
    "    all_features=all_features,\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "\n",
    "    if \"product_id\" in id_field:\n",
    "        concat_list = raw_data[0].map(\n",
    "            lambda example: {\n",
    "                \"concat_id\": f\"{example[id_field]}_|_{example[text_field]}_|_{example['product_title']}\"\n",
    "            },\n",
    "            remove_columns=all_features,\n",
    "        )\n",
    "        unique_item_list = concat_list.unique(\"concat_id\")\n",
    "        filtered_dataset_dict = {\n",
    "            \"_id\": [example.split(\"_|_\")[0] for example in unique_item_list],\n",
    "            \"title\": [example.split(\"_|_\")[2] for example in unique_item_list],\n",
    "            \"text\": [example.split(\"_|_\")[1] for example in unique_item_list],\n",
    "        }\n",
    "        filtered_dataset = pd.DataFrame(filtered_dataset_dict)\n",
    "    else:\n",
    "        concat_list = raw_data[0].map(\n",
    "            lambda example: {\n",
    "                \"concat_id\": f\"{example[id_field]}_|_{example[text_field]}\"\n",
    "            },\n",
    "            remove_columns=all_features,\n",
    "        )\n",
    "        unique_item_list = concat_list.unique(\"concat_id\")\n",
    "        filtered_dataset_dict = {\n",
    "            \"_id\": [example.split(\"_|_\")[0] for example in unique_item_list],\n",
    "            \"text\": [example.split(\"_|_\")[1] for example in unique_item_list],\n",
    "        }\n",
    "        filtered_dataset = pd.DataFrame(filtered_dataset_dict)\n",
    "\n",
    "    with open(jsonl_path, \"w\") as f:\n",
    "        f.write(filtered_dataset.to_json(lines=True, orient=\"records\"))\n",
    "\n",
    "\n",
    "def create_jsonl_training_labels(\n",
    "    tsv_train_path: str,\n",
    "    tsv_test_path: str,\n",
    "    raw_data=raw_data,\n",
    "    all_features: List[str] = all_features,\n",
    ") -> None:\n",
    "    train_data = raw_data[0].map(\n",
    "        lambda example: {\n",
    "            \"query-id\": str(example[\"query_id\"]),\n",
    "            \"corpus-id\": str(example[\"product_id\"]),\n",
    "            \"score\": 1,\n",
    "        },\n",
    "        remove_columns=all_features,\n",
    "    )\n",
    "    test_data = raw_data[1].map(\n",
    "        lambda example: {\n",
    "            \"query-id\": str(example[\"query_id\"]),\n",
    "            \"corpus-id\": str(example[\"product_id\"]),\n",
    "            \"score\": 1,\n",
    "        },\n",
    "        remove_columns=all_features,\n",
    "    )\n",
    "    train_data.to_csv(tsv_train_path, sep=\"\\t\")\n",
    "    test_data.to_csv(tsv_test_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### For large file ops we will use gcs fuse to save directly to the bucket\n",
    "\n",
    "`gcsfuse $BUCKET $data_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#establish mount point\n",
    "data_path = 'tuning_data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "!gcsfuse $BUCKET_NAME $data_path  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create gcs subfolder - why? this ensures we are not in the root for this and keeping the artifacts organized\n",
    "if not os.path.exists(os.path.join(data_path, data_path)):\n",
    "    os.mkdir(os.path.join(data_path, data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the corpus file https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings#prepare-tuning\n",
    "create_jsonl_from_raw_data(\n",
    "    jsonl_path=f\"{data_path}/{data_path}/query.jsonl\",\n",
    "    id_field=\"query_id\",\n",
    "    text_field=\"query\",\n",
    ")\n",
    "\n",
    "# the query file\n",
    "create_jsonl_from_raw_data(\n",
    "    jsonl_path=f\"/home/user/embedding_tensorboard_text_gecko_amazon_shopping/notebooks/tuning_data/{data_path}/corpus.jsonl\",\n",
    "    id_field=\"product_id\",\n",
    "    text_field=\"product_text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lastly, get training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_jsonl_training_labels(\n",
    "    tsv_train_path=f\"{data_path}/{data_path}/corpus-train.TSV\",\n",
    "    tsv_test_path=f\"{data_path}/{data_path}/corpus-test.TSV\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set tuning parameters in cell below\n",
    "\n",
    "Many of these are copy/paste from settings in first cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "BUCKET='gs://ecomm-query-product-pairs'\n",
    "PROJECT_ID=wortz-project-352116\n",
    "BASE_MODEL_VERSION_ID='textembedding-gecko@003'\n",
    "TASK_TYPE=SEMANTIC_SIMILARITY\n",
    "PIPELINE_SCRATCH_PATH=${BUCKET}\n",
    "QUERIES_PATH=${BUCKET}/tuning_data/query.jsonl\n",
    "CORPUS_PATH=${BUCKET}/tuning_data/corpus.jsonl\n",
    "TRAIN_LABEL_PATH=${BUCKET}/tuning_data/corpus-train.TSV\n",
    "TEST_LABEL_PATH=${BUCKET}/tuning_data/corpus-test.TSV\n",
    "BATCH_SIZE=100\n",
    "ITERATIONS=1000\n",
    "\n",
    "\n",
    "curl -X POST  \\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "\"https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/pipelineJobs?pipelineJobId=tune-text-embedding-$(date +%Y%m%d%H%M%S)\" \\\n",
    "-d '{\n",
    "  \"displayName\": \"tune-text-embedding-model\",\n",
    "  \"runtimeConfig\": {\n",
    "    \"gcsOutputDirectory\": \"'${PIPELINE_SCRATCH_PATH}'\",\n",
    "    \"parameterValues\": {\n",
    "      \"project\":  \"'${PROJECT_ID}'\",\n",
    "      \"base_model_version_id\":  \"'${BASE_MODEL_VERSION_ID}'\",\n",
    "      \"task_type\": \"'${TASK_TYPE}'\",\n",
    "      \"location\": \"us-central1\",\n",
    "      \"queries_path\":  \"'${QUERIES_PATH}'\",\n",
    "      \"corpus_path\":  \"'${CORPUS_PATH}'\",\n",
    "      \"train_label_path\":  \"'${TRAIN_LABEL_PATH}'\",\n",
    "      \"test_label_path\":  \"'${TEST_LABEL_PATH}'\",\n",
    "      \"batch_size\":  \"'${BATCH_SIZE}'\",\n",
    "      \"iterations\":  \"'${ITERATIONS}'\"\n",
    "    }\n",
    "  },\n",
    "  \"templateUri\": \"https://us-kfp.pkg.dev/ml-pipeline/llm-text-embedding/tune-text-embedding-model/v1.1.1\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_fine_tuning_dataframe(raw_data: datasets.dataset_dict.DatasetDict, unique_queries: Dict, unique_products: Dict, split: str = 'train') -> Tuple[pd.Dataframe, pd.Dataframe]:\n",
    "#     ### https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings#generative-ai-tune-embedding-drest\n",
    "#     data_dict_corpus = {\"_id\": [], \"text\": []}\n",
    "#     data_dict_query = data_dict_corpus.copy()\n",
    "#     raw_data = raw_data[split]\n",
    "#     for row in raw_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dataframe(\n",
    "    raw_data: Dict,\n",
    "    user_prompt: str = USER_PROMPT,\n",
    "    product_prompt: str = PRODUCT_PROMPT,\n",
    "    limit: int = LIMIT,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function returns batch prediction data for embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    for i, row in enumerate(raw_data[\"train\"]):\n",
    "        if i == limit - 1:\n",
    "            break\n",
    "        elif i == 0:\n",
    "            query_prod_pairs = pd.DataFrame(\n",
    "                {\n",
    "                    \"content\": [f'{user_prompt}{row[\"query\"]}'],\n",
    "                    \"type\": [\"query\"],\n",
    "                    \"id\": [row[\"query_id\"]],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            query_prod_pairs = pd.concat(\n",
    "                [\n",
    "                    query_prod_pairs,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"content\": [f'{user_prompt}{row[\"query\"]}'],\n",
    "                            \"type\": [\"query\"],\n",
    "                            \"id\": [row[\"query_id\"]],\n",
    "                        }\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        query_prod_pairs = pd.concat(\n",
    "            [\n",
    "                query_prod_pairs,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"content\": [f'{product_prompt}{row[\"product_title\"]}'],\n",
    "                        \"type\": [\"product_title\"],\n",
    "                        \"id\": [row[\"product_id\"]],\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    return query_prod_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prod_pairs = get_input_dataframe(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prod_pairs = query_prod_pairs.reset_index()\n",
    "query_prod_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get unique product ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch prediction\n",
    "\n",
    "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/batch-prediction-genai-embeddings#request_a_batch_response\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb $BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"batch_prediction_inputs.jsonl\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(query_prod_pairs[[\"content\"]].to_json(lines=True, orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp $output_file $BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "now_string_tag = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(\"Tag for this run: \", now_string_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "\n",
    "textembedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko\")\n",
    "batch_prediction_job = textembedding_model.batch_predict(\n",
    "    dataset=[f\"{BUCKET}/{output_file}\"],\n",
    "    destination_uri_prefix=f\"{BUCKET}/batch-predict-{now_string_tag}\",\n",
    ")\n",
    "print(batch_prediction_job.display_name)\n",
    "print(batch_prediction_job.resource_name)\n",
    "print(batch_prediction_job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When complete you should see something like this\n",
    "\n",
    "<img src='../img/bp-job.png' width=600px />\n",
    "\n",
    "<img src='../img/output-data.png' width=600px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the embeddings with Tensorboard\n",
    "\n",
    "Following this guide https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_output_gcs_folder = batch_prediction_job.output_info.gcs_output_directory\n",
    "\n",
    "! gsutil cp $bp_output_gcs_folder/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_json(path_or_buf=\"000000000000.jsonl\", lines=True)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(df: pd.DataFrame) -> List[List[float]]:\n",
    "    embedding_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        single_emb = row[\"predictions\"][0][\"embeddings\"][\"values\"]\n",
    "        embedding_list.append(single_emb)\n",
    "    return embedding_list\n",
    "\n",
    "\n",
    "embedding_list = get_predictions(predictions)\n",
    "len(embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a logs directory, so Tensorboard knows where to look for files.\n",
    "log_dir = \"logs/ecomm-example/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Save Labels separately on a line-by-line manner.\n",
    "with open(os.path.join(log_dir, \"metadata.tsv\"), \"w\") as f:\n",
    "    # header for columns\n",
    "    f.write(\"data_type\\tdata\\n\")\n",
    "    for instance in predictions.instance:\n",
    "        data_type = instance[\"content\"].split(\": \")[0]\n",
    "        # data_type = data_type\n",
    "        data = \"\".join(instance[\"content\"].split(\": \")[1:])\n",
    "        f.write(f\"{data_type}\\t{data}\\n\")\n",
    "\n",
    "\n",
    "# Save the weights we want to analyze as a variable. Note that the first\n",
    "# value represents any unknown word, which is not in the metadata, here\n",
    "# we will remove this value.\n",
    "weights = tf.Variable(embedding_list)\n",
    "# Create a checkpoint from embedding, the filename and key are the\n",
    "# name of the tensor.\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "# Set up config.\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = \"metadata.tsv\"\n",
    "projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run tensorboard against on log data we just saved.\n",
    "%tensorboard --logdir logs/ecomm-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above will run until you stop it\n",
    "\n",
    "You should be able to investigate the embedding space via PCA. Note the total variance captured to understand how much information is captured from the 3d view\n",
    "\n",
    "<img src=\"../img/tensorboard.png\" width=600px />\n",
    "\n",
    "\n",
    "#### Also a great way to understand performance is to select a point of interest and top k neighbors appear\n",
    "\n",
    "Below, we see natural hair dye query and it's associated nearest product description in the embedding space:\n",
    "\n",
    "\n",
    "<img src=\"../img/knn-analysis.png\" width=900px />\n",
    "\n",
    "\n",
    "#### Lastly, you can analyze and color by data type to get a feel for how well the queries relate to the products\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../img/analysis-by-type.png\" width=900px />\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorboard-four-tensorboard-four",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "display_name": "tensorboard-four",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

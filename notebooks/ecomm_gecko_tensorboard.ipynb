{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting batch prediction EDA with Tensor Board and Text Gecko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/tensorboard-four/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-23 19:02:14.310616: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-23 19:02:14.316797: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-23 19:02:14.388877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-23 19:02:15.855511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from datasets import load_dataset\n",
    "from typing import Dict, List\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "LIMIT = 3000\n",
    "PROJECT_ID = \"wortz-project-352116\"\n",
    "DATASET = \"ecomm-embedding\"\n",
    "BUCKET_NAME = \"ecomm-query-product-pairs\"\n",
    "BUCKET = f\"gs://{BUCKET_NAME}\"\n",
    "USER_PROMPT = \"User query: \"\n",
    "PRODUCT_PROMPT = \"Product title: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_dataset(\"tasksource/esci\")  # , split=['train[:10%]','test[:10%]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick examination of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features:  14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['example_id',\n",
       " 'query',\n",
       " 'query_id',\n",
       " 'product_id',\n",
       " 'product_locale',\n",
       " 'esci_label',\n",
       " 'small_version',\n",
       " 'large_version',\n",
       " 'product_title',\n",
       " 'product_description',\n",
       " 'product_bullet_point',\n",
       " 'product_brand',\n",
       " 'product_color',\n",
       " 'product_text']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total features: \", len(raw_data.column_names[\"train\"]))\n",
    "all_features = raw_data.column_names[\"train\"]\n",
    "all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guide for embedding fine tuning\n",
    "\n",
    "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# raw_data = load_dataset(\"tasksource/esci\", split=['train[:20%]','test[:10%]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_jsonl_from_raw_data(\n",
    "    jsonl_path: str,\n",
    "    id_field: str,\n",
    "    text_field: str,\n",
    "    raw_data=raw_data,\n",
    "    all_features=all_features,\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "\n",
    "    if \"product_id\" in id_field:\n",
    "        concat_list = raw_data[\"train\"].map(\n",
    "            lambda example: {\n",
    "                \"concat_id\": f\"{example[id_field]}_|_{example[text_field]}_|_{example['product_title']}\"\n",
    "            },\n",
    "            remove_columns=all_features,\n",
    "        )\n",
    "        unique_item_list = concat_list.unique(\"concat_id\")\n",
    "        filtered_dataset_dict = {\n",
    "            \"_id\": [example.split(\"_|_\")[0] for example in unique_item_list],\n",
    "            \"title\": [example.split(\"_|_\")[2] for example in unique_item_list],\n",
    "            \"text\": [example.split(\"_|_\")[1] for example in unique_item_list],\n",
    "        }\n",
    "        filtered_dataset = pd.DataFrame(filtered_dataset_dict)\n",
    "    else:\n",
    "        concat_list = raw_data[\"train\"].map(\n",
    "            lambda example: {\n",
    "                \"concat_id\": f\"{example[id_field]}_|_{example[text_field]}\"\n",
    "            },\n",
    "            remove_columns=all_features,\n",
    "        )\n",
    "        unique_item_list = concat_list.unique(\"concat_id\")\n",
    "        filtered_dataset_dict = {\n",
    "            \"_id\": [example.split(\"_|_\")[0] for example in unique_item_list],\n",
    "            \"text\": [example.split(\"_|_\")[1] for example in unique_item_list],\n",
    "        }\n",
    "        filtered_dataset = pd.DataFrame(filtered_dataset_dict)\n",
    "\n",
    "    with open(jsonl_path, \"w\") as f:\n",
    "        f.write(filtered_dataset.to_json(lines=True, orient=\"records\"))\n",
    "\n",
    "\n",
    "def create_jsonl_training_labels(\n",
    "    tsv_train_path: str,\n",
    "    tsv_test_path: str,\n",
    "    raw_data=raw_data,\n",
    "    all_features: List[str] = all_features,\n",
    ") -> None:\n",
    "    train_data = raw_data[\"train\"].map(\n",
    "        lambda example: {\n",
    "            \"query-id\": str(example[\"query_id\"]),\n",
    "            \"corpus-id\": str(example[\"product_id\"]),\n",
    "            \"score\": 1,\n",
    "        },\n",
    "        remove_columns=all_features,\n",
    "    )\n",
    "    test_data = raw_data[\"test\"].map(\n",
    "        lambda example: {\n",
    "            \"query-id\": str(example[\"query_id\"]),\n",
    "            \"corpus-id\": str(example[\"product_id\"]),\n",
    "            \"score\": 1,\n",
    "        },\n",
    "        remove_columns=all_features,\n",
    "    )\n",
    "    train_data.to_csv(tsv_train_path, sep=\"\\t\")\n",
    "    test_data.to_csv(tsv_test_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### For large file ops we will use gcs fuse to save directly to the bucket\n",
    "\n",
    "`gcsfuse $BUCKET $data_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"23/03/2024 07:02:28.735879\",\"severity\":\"INFO\",\"message\":\"Start gcsfuse/2.0.0 (Go version go1.22.1) for app \\\"\\\" using mount point: /home/user/embedding_tensorboard_text_gecko_amazon_shopping/notebooks/tuning_data\\n\"}\n",
      "{\"time\":\"23/03/2024 07:02:28.736284\",\"severity\":\"INFO\",\"message\":\"GCSFuse mount command flags: {\\\"AppName\\\":\\\"\\\",\\\"Foreground\\\":false,\\\"ConfigFile\\\":\\\"\\\",\\\"MountOptions\\\":{},\\\"DirMode\\\":493,\\\"FileMode\\\":420,\\\"Uid\\\":-1,\\\"Gid\\\":-1,\\\"ImplicitDirs\\\":false,\\\"OnlyDir\\\":\\\"\\\",\\\"RenameDirLimit\\\":0,\\\"CustomEndpoint\\\":null,\\\"BillingProject\\\":\\\"\\\",\\\"KeyFile\\\":\\\"\\\",\\\"TokenUrl\\\":\\\"\\\",\\\"ReuseTokenFromUrl\\\":true,\\\"EgressBandwidthLimitBytesPerSecond\\\":-1,\\\"OpRateLimitHz\\\":-1,\\\"SequentialReadSizeMb\\\":200,\\\"MaxRetrySleep\\\":30000000000,\\\"StatCacheCapacity\\\":20460,\\\"StatCacheTTL\\\":60000000000,\\\"TypeCacheTTL\\\":60000000000,\\\"HttpClientTimeout\\\":0,\\\"MaxRetryDuration\\\":-1000000000,\\\"RetryMultiplier\\\":2,\\\"LocalFileCache\\\":false,\\\"TempDir\\\":\\\"\\\",\\\"ClientProtocol\\\":\\\"http1\\\",\\\"MaxConnsPerHost\\\":100,\\\"MaxIdleConnsPerHost\\\":100,\\\"EnableNonexistentTypeCache\\\":false,\\\"StackdriverExportInterval\\\":0,\\\"OtelCollectorAddress\\\":\\\"\\\",\\\"LogFile\\\":\\\"\\\",\\\"LogFormat\\\":\\\"json\\\",\\\"ExperimentalEnableJsonRead\\\":false,\\\"DebugFuseErrors\\\":true,\\\"DebugFuse\\\":false,\\\"DebugFS\\\":false,\\\"DebugGCS\\\":false,\\\"DebugHTTP\\\":false,\\\"DebugInvariants\\\":false,\\\"DebugMutex\\\":false}\"}\n",
      "{\"time\":\"23/03/2024 07:02:28.736429\",\"severity\":\"INFO\",\"message\":\"GCSFuse mount config flags: {\\\"CreateEmptyFile\\\":false,\\\"Severity\\\":\\\"INFO\\\",\\\"Format\\\":\\\"json\\\",\\\"FilePath\\\":\\\"\\\",\\\"LogRotateConfig\\\":{\\\"MaxFileSizeMB\\\":512,\\\"BackupFileCount\\\":10,\\\"Compress\\\":true},\\\"MaxSizeMB\\\":-1,\\\"CacheFileForRangeRead\\\":false,\\\"CacheDir\\\":\\\"\\\",\\\"TtlInSeconds\\\":-9223372036854775808,\\\"TypeCacheMaxSizeMB\\\":4,\\\"StatCacheMaxSizeMB\\\":-9223372036854775808,\\\"EnableEmptyManagedFolders\\\":false}\"}\n",
      "{\"time\":\"23/03/2024 07:02:28.973997\",\"severity\":\"INFO\",\"message\":\"File system has been successfully mounted.\"}\n"
     ]
    }
   ],
   "source": [
    "#establish mount point\n",
    "data_path = 'tuning_data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "!gcsfuse $BUCKET_NAME $data_path  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create gcs subfolder - why? this ensures we are not in the root for this and keeping the artifacts organized\n",
    "if not os.path.exists(os.path.join(data_path,data_path)):\n",
    "    os.mkdir(os.path.join(data_path,data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  22%|██▏       | 441087/2027874 [00:57<03:22, 7826.96 examples/s]"
     ]
    }
   ],
   "source": [
    "# the corpus file https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings#prepare-tuning\n",
    "create_jsonl_from_raw_data(\n",
    "    jsonl_path=f\"{data_path}/{data_path}/query.jsonl\", id_field=\"query_id\", text_field=\"query\"\n",
    ")\n",
    "\n",
    "# the query file\n",
    "create_jsonl_from_raw_data(\n",
    "    jsonl_path=f\"{data_path}/{data_path}/corpus.jsonl\",\n",
    "    id_field=\"product_id\",\n",
    "    text_field=\"product_text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lastly, get training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_jsonl_training_labels(\n",
    "    tsv_train_path=f\"{data_path}/{data_path}/corpus-train.TSV\",\n",
    "    tsv_test_path=f\"{data_path}/{data_path}/corpus-test.TSV\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send the data up to gcs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! gsutil cp -r $data_path/* $BUCKET/tuning_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set tuning parameters in cell below\n",
    "\n",
    "Many of these are copy/paste from settings in first cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "BUCKET='gs://ecomm-query-product-pairs'\n",
    "PROJECT_ID=wortz-project-352116\n",
    "BASE_MODEL_VERSION_ID='textembedding-gecko@003'\n",
    "TASK_TYPE=SEMANTIC_SIMILARITY\n",
    "PIPELINE_SCRATCH_PATH=${BUCKET}\n",
    "QUERIES_PATH=${BUCKET}/tuning_data/query.jsonl\n",
    "CORPUS_PATH=${BUCKET}/tuning_data/corpus.jsonl\n",
    "TRAIN_LABEL_PATH=${BUCKET}/tuning_data/corpus-train.TSV\n",
    "TEST_LABEL_PATH=${BUCKET}/tuning_data/corpus-test.TSV\n",
    "BATCH_SIZE=100\n",
    "ITERATIONS=1000\n",
    "\n",
    "\n",
    "curl -X POST  \\\n",
    "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "\"https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/pipelineJobs?pipelineJobId=tune-text-embedding-$(date +%Y%m%d%H%M%S)\" \\\n",
    "-d '{\n",
    "  \"displayName\": \"tune-text-embedding-model\",\n",
    "  \"runtimeConfig\": {\n",
    "    \"gcsOutputDirectory\": \"'${PIPELINE_SCRATCH_PATH}'\",\n",
    "    \"parameterValues\": {\n",
    "      \"project\":  \"'${PROJECT_ID}'\",\n",
    "      \"base_model_version_id\":  \"'${BASE_MODEL_VERSION_ID}'\",\n",
    "      \"task_type\": \"'${TASK_TYPE}'\",\n",
    "      \"location\": \"us-central1\",\n",
    "      \"queries_path\":  \"'${QUERIES_PATH}'\",\n",
    "      \"corpus_path\":  \"'${CORPUS_PATH}'\",\n",
    "      \"train_label_path\":  \"'${TRAIN_LABEL_PATH}'\",\n",
    "      \"test_label_path\":  \"'${TEST_LABEL_PATH}'\",\n",
    "      \"batch_size\":  \"'${BATCH_SIZE}'\",\n",
    "      \"iterations\":  \"'${ITERATIONS}'\"\n",
    "    }\n",
    "  },\n",
    "  \"templateUri\": \"https://us-kfp.pkg.dev/ml-pipeline/llm-text-embedding/tune-text-embedding-model/v1.1.1\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_fine_tuning_dataframe(raw_data: datasets.dataset_dict.DatasetDict, unique_queries: Dict, unique_products: Dict, split: str = 'train') -> Tuple[pd.Dataframe, pd.Dataframe]:\n",
    "#     ### https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings#generative-ai-tune-embedding-drest\n",
    "#     data_dict_corpus = {\"_id\": [], \"text\": []}\n",
    "#     data_dict_query = data_dict_corpus.copy()\n",
    "#     raw_data = raw_data[split]\n",
    "#     for row in raw_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dataframe(\n",
    "    raw_data: Dict,\n",
    "    user_prompt: str = USER_PROMPT,\n",
    "    product_prompt: str = PRODUCT_PROMPT,\n",
    "    limit: int = LIMIT,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function returns batch prediction data for embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    for i, row in enumerate(raw_data[\"train\"]):\n",
    "        if i == limit - 1:\n",
    "            break\n",
    "        elif i == 0:\n",
    "            query_prod_pairs = pd.DataFrame(\n",
    "                {\n",
    "                    \"content\": [f'{user_prompt}{row[\"query\"]}'],\n",
    "                    \"type\": [\"query\"],\n",
    "                    \"id\": [row[\"query_id\"]],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            query_prod_pairs = pd.concat(\n",
    "                [\n",
    "                    query_prod_pairs,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"content\": [f'{user_prompt}{row[\"query\"]}'],\n",
    "                            \"type\": [\"query\"],\n",
    "                            \"id\": [row[\"query_id\"]],\n",
    "                        }\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        query_prod_pairs = pd.concat(\n",
    "            [\n",
    "                query_prod_pairs,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"content\": [f'{product_prompt}{row[\"product_title\"]}'],\n",
    "                        \"type\": [\"product_title\"],\n",
    "                        \"id\": [row[\"product_id\"]],\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    return query_prod_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prod_pairs = get_input_dataframe(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>User query:  revent 80 cfm</td>\n",
       "      <td>query</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Product title: Panasonic FV-20VQ3 WhisperCeili...</td>\n",
       "      <td>product_title</td>\n",
       "      <td>B000MOO21W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>User query: bathroom fan without light</td>\n",
       "      <td>query</td>\n",
       "      <td>13723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Product title: Panasonic FV-20VQ3 WhisperCeili...</td>\n",
       "      <td>product_title</td>\n",
       "      <td>B000MOO21W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>User query:  revent 80 cfm</td>\n",
       "      <td>query</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            content           type  \\\n",
       "0      0                         User query:  revent 80 cfm          query   \n",
       "1      0  Product title: Panasonic FV-20VQ3 WhisperCeili...  product_title   \n",
       "2      0             User query: bathroom fan without light          query   \n",
       "3      0  Product title: Panasonic FV-20VQ3 WhisperCeili...  product_title   \n",
       "4      0                         User query:  revent 80 cfm          query   \n",
       "\n",
       "           id  \n",
       "0           0  \n",
       "1  B000MOO21W  \n",
       "2       13723  \n",
       "3  B000MOO21W  \n",
       "4           0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_prod_pairs = query_prod_pairs.reset_index()\n",
    "query_prod_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get unique product ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch prediction\n",
    "\n",
    "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/batch-prediction-genai-embeddings#request_a_batch_response\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb $BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"batch_prediction_inputs.jsonl\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(query_prod_pairs[[\"content\"]].to_json(lines=True, orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp $output_file $BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "now_string_tag = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(\"Tag for this run: \", now_string_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "\n",
    "textembedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko\")\n",
    "batch_prediction_job = textembedding_model.batch_predict(\n",
    "    dataset=[f\"{BUCKET}/{output_file}\"],\n",
    "    destination_uri_prefix=f\"{BUCKET}/batch-predict-{now_string_tag}\",\n",
    ")\n",
    "print(batch_prediction_job.display_name)\n",
    "print(batch_prediction_job.resource_name)\n",
    "print(batch_prediction_job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When complete you should see something like this\n",
    "\n",
    "<img src='../img/bp-job.png' width=600px />\n",
    "\n",
    "<img src='../img/output-data.png' width=600px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the embeddings with Tensorboard\n",
    "\n",
    "Following this guide https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_output_gcs_folder = batch_prediction_job.output_info.gcs_output_directory\n",
    "\n",
    "! gsutil cp $bp_output_gcs_folder/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_json(path_or_buf=\"000000000000.jsonl\", lines=True)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(df: pd.DataFrame) -> List[List[float]]:\n",
    "    embedding_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        single_emb = row[\"predictions\"][0][\"embeddings\"][\"values\"]\n",
    "        embedding_list.append(single_emb)\n",
    "    return embedding_list\n",
    "\n",
    "\n",
    "embedding_list = get_predictions(predictions)\n",
    "len(embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a logs directory, so Tensorboard knows where to look for files.\n",
    "log_dir = \"logs/ecomm-example/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Save Labels separately on a line-by-line manner.\n",
    "with open(os.path.join(log_dir, \"metadata.tsv\"), \"w\") as f:\n",
    "    # header for columns\n",
    "    f.write(\"data_type\\tdata\\n\")\n",
    "    for instance in predictions.instance:\n",
    "        data_type = instance[\"content\"].split(\": \")[0]\n",
    "        # data_type = data_type\n",
    "        data = \"\".join(instance[\"content\"].split(\": \")[1:])\n",
    "        f.write(f\"{data_type}\\t{data}\\n\")\n",
    "\n",
    "\n",
    "# Save the weights we want to analyze as a variable. Note that the first\n",
    "# value represents any unknown word, which is not in the metadata, here\n",
    "# we will remove this value.\n",
    "weights = tf.Variable(embedding_list)\n",
    "# Create a checkpoint from embedding, the filename and key are the\n",
    "# name of the tensor.\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "# Set up config.\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = \"metadata.tsv\"\n",
    "projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 30352), started 0:08:44 ago. (Use '!kill 30352' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-eb0817d48ad4c9b8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-eb0817d48ad4c9b8\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now run tensorboard against on log data we just saved.\n",
    "%tensorboard --logdir logs/ecomm-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above will run until you stop it\n",
    "\n",
    "You should be able to investigate the embedding space via PCA. Note the total variance captured to understand how much information is captured from the 3d view\n",
    "\n",
    "<img src=\"../img/tensorboard.png\" width=600px />\n",
    "\n",
    "\n",
    "#### Also a great way to understand performance is to select a point of interest and top k neighbors appear\n",
    "\n",
    "Below, we see natural hair dye query and it's associated nearest product description in the embedding space:\n",
    "\n",
    "\n",
    "<img src=\"../img/knn-analysis.png\" width=900px />\n",
    "\n",
    "\n",
    "#### Lastly, you can analyze and color by data type to get a feel for how well the queries relate to the products\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../img/analysis-by-type.png\" width=900px />\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorboard-four-tensorboard-four",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "display_name": "tensorboard-four",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
